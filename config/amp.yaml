seed: ${trial_id}
trial_id: 0
project_name: bioseqgfn_al
version: v0.0.1

# Directories for loading and storing data
data_dir: /home/mila/m/moksh.jain/scratch/amp
exp_name: test
group_name: somedetails
exp_tags: []
job_name: null
timestamp: ${now:%Y-%m-%d_%H-%M-%S}
log_dir: ${data_dir}/${exp_name}
wandb_mode: disabled
data_path: 'data/'
num_samples: 5000
k: 100
load_proxy_path: null
use_offset: False

hydra:
  run:
    dir: ${log_dir}
  sweep:
    dir: ${log_dir}
    subdir: .

dataset:
  _target_: lib.dataset.amp.AMPClassificationDataset
  split: "D"
  nfold: 5

tokenizer:
  _target_: lib.utils.tokenizers.ResidueTokenizer

gfn:
  random_action_prob: 0.01
  max_len: 60
  min_len: 12
  batch_size: 16
  reward_min: 1e-20
  sampling_temp: 1
  train_steps: 20000
  pi_lr: 0.005
  z_lr: 0.01
  wd: 0.0001
  gen_clip: 10
  sample_beta: 3
  eval_freq: 100
  k: 100
  eval_batch_size: 64
  data_per_step: 16
  
  model:
    _target_: lib.model.transformer.GFNTransformer
    max_len: 60
    vocab_size: 26
    num_actions: 21
    num_hid: 64
    num_layers: 3
    num_head: 8
    dropout: 0
    partition_init: 150

proxy:
  _target_: lib.proxy.regression.TransformerDropout
  learning_rate: 1e-4
  num_layers: 4
  L2: 1e-6
  batch_size: 256
  early_stop_tol: 15
  train_steps: 100000
  num_dropout_samples: 25
  max_len: 65
  classification: True
  early_stop_to_best_params: True
  save_path: 'proxy.pt'  
  model: 
    _target_: lib.model.transformer.Transformer
    max_len: 239
    vocab_size: 26
    num_outputs: 1
    num_hid: 64
    num_layers: 4
    num_head: 8
    dropout: 0