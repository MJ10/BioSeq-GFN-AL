seed: ${trial_id}
trial_id: 0
project_name: bioseqgfn_al
version: v0.0.1

# Directories for loading and storing data
data_dir: /home/mila/m/moksh.jain/scratch/amp
exp_name: test
group_name: somedetails
exp_tags: []
job_name: null
timestamp: ${now:%Y-%m-%d_%H-%M-%S}
log_dir: ${data_dir}/${exp_name}
wandb_mode: disabled
data_path: 'data/'
num_samples: 1000
k: 100
load_proxy_path: null
use_offset: True

hydra:
  run:
    dir: ${log_dir}
  sweep:
    dir: ${log_dir}
    subdir: .

dataset:
  _target_: lib.dataset.amp.AMPExperimentalDataset
  path: "~/BioSeq-GFN-AL/data/"
  load_cache: False
  save_cache: True
  cache_path: "exp_cache.pkl"
  # medium: "plate"

tokenizer:
  _target_: lib.utils.tokenizers.ResidueTokenizer

gfn:
  random_action_prob: 0.1
  max_len: 60
  min_len: 60
  batch_size: 128
  reward_min: 1e-20
  sampling_temp: 1
  train_steps: 10000
  pi_lr: 0.0001
  z_lr: 0.001
  wd: 0.0001
  gen_clip: 10
  sample_beta: 8
  eval_freq: 100
  k: 100
  num_samples: 5000
  data_per_step: 16
  val_batch_size: 256
  eval_batch_size: 64
  eval_samples: 512
  
  offline_gamma: 0.2
  
  model:
    _target_: lib.model.transformer.GFNTransformer
    max_len: 237
    vocab_size: 26
    num_actions: 21
    num_hid: 128
    num_layers: 3
    num_head: 8
    dropout: 0
    partition_init: 150

proxy:
  _target_: lib.proxy.lvm.LVMN
  num_layers: 4
  L2: 1e-6
  batch_size: 128
  early_stop_tol: 5
  train_steps: 100000
  num_dropout_samples: 25
  max_len: 65
  gfn: False
  p_lr: 1e-4
  q_lr: 1e-4
  classification: False
  early_stop_to_best_params: True
  save_path: null
  model_p_o:
    _target_: lib.model.transformer.Transformer
    max_len: 65
    vocab_size: 26
    num_outputs: 1
    num_hid: 128
    num_layers: 3
    num_head: 8
    dropout: 0
  
  model_q_o: 
    _target_: lib.model.transformer.TransformerLVM
    max_len: 65
    vocab_size: 26
    num_outputs: 1
    num_hid: 64
    num_layers: 3
    num_head: 8
    dropout: 0
    num_aux: 8
    
  model_p_c: 
    _target_: lib.model.transformer.MLP
    in_dim: 8
    out_dim: 1
    hidden_layers: [64, 64]
    dropout_prob: 0.1